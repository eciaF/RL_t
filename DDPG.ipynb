{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from gym import wrappers\n",
    "from torch.autograd import Variable\n",
    "from collections import deque\n",
    "from tensorboardX import SummaryWriter\n",
    "\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('param.json') as f:\n",
    "    config = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OrnsteinUhlenbeckProcess:\n",
    "    def __init__(self, mu=np.zeros(1), sigma=0.05, theta=.25, dimension=1e-2, x0=None, num_steps=12000):\n",
    "        self.theta = theta\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.dt = dimension\n",
    "        self.x0 = x0\n",
    "        self.reset()\n",
    "\n",
    "    def step(self):\n",
    "        x = self.x_prev + self.theta * (self.mu - self.x_prev) * self.dt + \\\n",
    "                self.sigma * np.sqrt(self.dt) * np.random.normal(size=self.mu.shape)\n",
    "        self.x_prev = x\n",
    "        return x\n",
    "\n",
    "    def reset(self):\n",
    "        self.x_prev = self.x0 if self.x0 is not None else np.zeros_like(self.mu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Actor(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1, fc2):\n",
    "        super(Actor, self).__init__()\n",
    "        # fci = fully connected i\n",
    "        self.layer1 = nn.Linear(state_size, fc1)\n",
    "        self.layer2 = nn.Linear(fc1, fc2)\n",
    "        self.layer3 = nn.Linear(fc2, action_size)\n",
    "        \n",
    "        self.leak = 0.01\n",
    "        \n",
    "        self.reset_parameteres()\n",
    "\n",
    "    def forward(self, states):\n",
    "        out_l1 = F.relu(self.layer1(states))\n",
    "        out_l2 = F.relu(self.layer2(out_l1))\n",
    "        out = F.tanh(self.layer3(out_l2))\n",
    "        #out = F.tanh(self.layer3(out_l2))\n",
    "        return out\n",
    "    \n",
    "    def reset_parameteres(self):\n",
    "        torch.nn.init.kaiming_normal_(self.layer1.weight.data, a=self.leak, mode='fan_in')\n",
    "        torch.nn.init.kaiming_normal_(self.layer2.weight.data, a=self.leak, mode='fan_in')\n",
    "        torch.nn.init.uniform_(self.layer3.weight.data, -3e-3, 3e-3)\n",
    "    \n",
    "    \n",
    "class Q_network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size, action_size, fc1, fc2):\n",
    "        super(Q_network, self).__init__()\n",
    "        # fci = fully connected i\n",
    "        self.layer1 = nn.Linear(state_size + action_size, fc1)\n",
    "        self.layer2 = nn.Linear(fc1, fc2)\n",
    "        self.layer3 = nn.Linear(fc2, 1)\n",
    "        \n",
    "        self.leak = 0.01\n",
    "        \n",
    "        self.reset_parameteres()\n",
    "\n",
    "    def forward(self, states, actions):\n",
    "        batch = torch.cat([states, actions], 1)\n",
    "        out_l1 = F.relu(self.layer1(batch))\n",
    "        out_l2 = F.relu(self.layer2(out_l1))\n",
    "        out = self.layer3(out_l2)\n",
    "        return out\n",
    "\n",
    "    def reset_parameteres(self):\n",
    "        torch.nn.init.kaiming_normal_(self.layer1.weight.data, a=self.leak, mode='fan_in')\n",
    "        torch.nn.init.kaiming_normal_(self.layer2.weight.data, a=self.leak, mode='fan_in')\n",
    "        torch.nn.init.uniform_(self.layer3.weight.data, -3e-3, 3e-3)    \n",
    "    \n",
    "class ReplayBuffer:\n",
    "\n",
    "    def __init__(self, state_size, action_size, capacity, device):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "\n",
    "        self.index = 0\n",
    "        self.full = False\n",
    "\n",
    "        self.state = np.empty(shape=(capacity, state_size), dtype=np.float32)\n",
    "        self.action = np.empty(shape=(capacity, action_size), dtype=np.float32)\n",
    "        self.next_state = np.empty(shape=(capacity, state_size), dtype=np.float32)\n",
    "        self.reward = np.empty(shape=(capacity, 1), dtype=np.int8)\n",
    "        self.done = np.empty(shape=(capacity, 1), dtype=np.int8)\n",
    "\n",
    "    def add(self, state, reward, action, next_state, done):\n",
    "        self.state[self.index] = state\n",
    "        self.action[self.index] = action\n",
    "        self.next_state[self.index] = next_state\n",
    "        self.reward[self.index] = reward\n",
    "        self.done[self.index] = done\n",
    "\n",
    "        self.index = (self.index + 1) % self.capacity\n",
    "        self.full = True if self.index == 0 else False\n",
    "\n",
    "    def sample(self, batchsize):\n",
    "        limit = self.index if not self.full else self.capacity - 1\n",
    "\n",
    "        batch = np.random.randint(0, limit, size=batchsize)\n",
    "\n",
    "        state = torch.as_tensor(self.state[batch])\n",
    "        action = torch.as_tensor(self.action[batch])\n",
    "        next_state = torch.as_tensor(self.next_state[batch])\n",
    "        reward = torch.as_tensor(self.reward[batch])\n",
    "        done = torch.as_tensor(self.done[batch])\n",
    "\n",
    "        return state, action, next_state, reward, done\n",
    "\n",
    "    def save_memory(self):\n",
    "        pass\n",
    "\n",
    "    def load_memory(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "\n",
    "    def __init__(self, action_size, state_size, config):\n",
    "        self.action_size = action_size\n",
    "        self.state_size = state_size\n",
    "        self.seed = config[\"seed\"]\n",
    "        self.lr_actor = config[\"lr_actor\"]\n",
    "        self.lr_critic = config[\"lr_critic\"]\n",
    "        self.tau = config[\"tau\"]\n",
    "        self.gamma = config[\"gamma\"]\n",
    "        self.batch_size = config[\"batch_size\"]\n",
    "        if config[\"device\"] == \"cuda\":\n",
    "            if torch.cuda.is_available():\n",
    "                config[\"device\"] == \"cpu\"\n",
    "        self.device = config[\"device\"]\n",
    "        \n",
    "        # initialize noise\n",
    "        self.noise = OrnsteinUhlenbeckProcess(sigma=0.2, theta=0.15)\n",
    "        self.noise.reset()\n",
    "        \n",
    "        # replay\n",
    "        self.memory = ReplayBuffer(state_size, action_size, config[\"buffer_size\"], self.device)\n",
    "        \n",
    "        # everything necessary for SummaryWriter\n",
    "        pathname = 8\n",
    "        tensorboard_name = str(config[\"locexp\"]) + '/runs' + str(pathname)\n",
    "        self.writer = SummaryWriter(tensorboard_name)\n",
    "        self.steps = 0\n",
    "\n",
    "        # set seeds\n",
    "        torch.manual_seed(self.seed)\n",
    "        np.random.seed(self.seed)\n",
    "\n",
    "        # actor, optimizer of actor, target for actor, critic, optimizer of critic, target for critic\n",
    "        self.actor = Actor(state_size, action_size, config[\"fc1_units\"], config[\"fc2_units\"]).to(self.device)\n",
    "        self.optimizer_a = torch.optim.Adam(self.actor.parameters(), self.lr_actor)\n",
    "        self.target_actor = Actor(state_size, action_size, config[\"fc1_units\"], config[\"fc2_units\"]).to(self.device)\n",
    "        self.target_actor.load_state_dict(self.actor.state_dict())\n",
    "        self.critic = Q_network(state_size, action_size, config[\"fc1_units\"], config[\"fc2_units\"]).to(self.device)\n",
    "        self.optimizer_q = torch.optim.Adam(self.critic.parameters(), self.lr_critic)\n",
    "        self.target_critic = Q_network(state_size, action_size, config[\"fc1_units\"], config[\"fc2_units\"]).to(self.device)\n",
    "        self.target_critic.load_state_dict(self.critic.state_dict())\n",
    "        \n",
    "    def act(self, state, greedy=False):\n",
    "        state = torch.as_tensor(state, dtype=torch.float32, device=torch.device(\"cpu\"))\n",
    "        state = state.unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            action = self.actor(state).numpy()[0]#.detach().numpy()\n",
    "        \n",
    "        # ^ torch.argmax(q_nns) in continuous case\n",
    "        noise = self.noise.step()\n",
    "        #print(noise)\n",
    "        action = action if greedy else np.clip(action + noise, -1, 1)#self.noise.step(), -1, 1)        \n",
    "        return action\n",
    "        \n",
    "    def train(self, episodes, timesteps):\n",
    "        #env = gym.make(\"MountainCarContinuous-v0\")\n",
    "        env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "        \n",
    "        mean_r = 0\n",
    "        mean_episode = 0\n",
    "        dq = deque(maxlen=100)\n",
    "        for i in range(1, episodes+1):\n",
    "            state = env.reset()\n",
    "            if i % 10 == 0:\n",
    "                self.noise.reset()\n",
    "            \n",
    "            for j in range(timesteps):\n",
    "                action = self.act(state)\n",
    "                next_state, reward, done, _ = env.step(action)\n",
    "                #next_state = np.squeeze(next_state, axis=1) bei Car\n",
    "                self.memory.add(state, reward, action, next_state, done)\n",
    "                state = next_state\n",
    "                mean_r += reward\n",
    "                \n",
    "                # fill replay buffer with 10 samples before updating the policy\n",
    "                if i > 10:\n",
    "                    self.update()\n",
    "                    \n",
    "                if done:\n",
    "                    print(f\"timesteps until break: {j}\")\n",
    "                    break\n",
    "            \n",
    "            # append mean of episode to list and add average of last 10 means if existent, then reset mean\n",
    "            #print(mean_r)\n",
    "            #print(type(mean_r))\n",
    "            dq.append(mean_r)\n",
    "            mean_episode = np.mean(dq)\n",
    "            self.writer.add_scalar(\"a_rew\", mean_episode, i)\n",
    "            \n",
    "            # print every tenth episode to keep track\n",
    "            print(f\"Episode: {i}, mean_r: {mean_r}, mean_episode: {mean_episode}\")\n",
    "            #if i % 10 == 0:\n",
    "            #    print(f\"Episode: {i}, mean_r: {mean_r}, mean_episode: {mean_episode}\")\n",
    "            \n",
    "            mean_r = 0\n",
    "\n",
    "    def update(self):\n",
    "        self.steps += 1\n",
    "        # sample minibatch and calculate target value and q_nns\n",
    "        state, action, next_state, reward, done = self.memory.sample(self.batch_size)\n",
    "        #print(state.shape)\n",
    "        #print(action.shape)\n",
    "        #print(next_state.shape)\n",
    "        #print(reward.shape)\n",
    "        #print(done.shape)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            next_action = self.target_actor(next_state)#.detach()#.numpy()\n",
    "            q_target = self.target_critic(next_state, next_action)\n",
    "            y_target = reward + (self.gamma * q_target * (1-done))\n",
    "        \n",
    "        #print(next_action.shape)\n",
    "        #print(q_target.shape)\n",
    "        #print(y_target.shape)\n",
    "        \n",
    "        # update critic\n",
    "        q_samples_target = self.critic(state, action)\n",
    "        loss_critic = F.mse_loss(q_samples_target, y_target)\n",
    "        self.writer.add_scalar(\"loss_critic\", loss_critic, self.steps)\n",
    "        \n",
    "        #print(q_samples_target.shape)\n",
    "        \n",
    "        # set gradients to zero and optimize q\n",
    "        self.optimizer_q.zero_grad()\n",
    "        loss_critic.backward()\n",
    "        self.optimizer_q.step\n",
    "        \n",
    "        # update actor\n",
    "        c_action = self.actor(state)\n",
    "        q_sum_samples = self.critic(state, c_action)\n",
    "        loss_actor = -q_sum_samples.mean()\n",
    "        self.writer.add_scalar(\"loss_actor\", loss_actor, self.steps)\n",
    "        \n",
    "        # set gradients to zero and optimize a\n",
    "        self.optimizer_a.zero_grad()\n",
    "        loss_actor.backward()        \n",
    "        self.optimizer_a.step()\n",
    "        \n",
    "        # update target networks\n",
    "        self.update_target(self.actor, self.target_actor)\n",
    "        self.update_target(self.critic, self.target_critic)\n",
    "        \n",
    "    def update_target(self, online, target):\n",
    "        for parameter, target in zip(online.parameters(), target.parameters()):\n",
    "            target.data.copy_(self.tau * parameter.data + (1 - self.tau) * target.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env = gym.make(\"MountainCarContinuous-v0\")\n",
    "env = gym.make(\"LunarLanderContinuous-v2\")\n",
    "#state = env.reset()\n",
    "action_space = env.action_space.shape[0]\n",
    "state_space = env.observation_space.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(action_size=action_space, state_size=state_space, config=config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ecia/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py:1628: UserWarning: nn.functional.tanh is deprecated. Use torch.tanh instead.\n",
      "  warnings.warn(\"nn.functional.tanh is deprecated. Use torch.tanh instead.\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps until break: 66\n",
      "Episode: 1, mean_r: -149.34171600738364, mean_episode: -149.34171600738364\n",
      "timesteps until break: 57\n",
      "Episode: 2, mean_r: -126.9462417048907, mean_episode: -138.14397885613715\n",
      "timesteps until break: 84\n",
      "Episode: 3, mean_r: -125.18388763153442, mean_episode: -133.82394844793623\n",
      "timesteps until break: 76\n",
      "Episode: 4, mean_r: -33.701864812680355, mean_episode: -108.79342753912226\n",
      "timesteps until break: 125\n",
      "Episode: 5, mean_r: -69.55352385453632, mean_episode: -100.94544680220507\n",
      "timesteps until break: 125\n",
      "Episode: 6, mean_r: -302.7105510685486, mean_episode: -134.57296417992902\n",
      "timesteps until break: 99\n",
      "Episode: 7, mean_r: -333.4336656086961, mean_episode: -162.98163581261002\n",
      "timesteps until break: 99\n",
      "Episode: 8, mean_r: -399.8545171660434, mean_episode: -192.59074598178918\n",
      "timesteps until break: 93\n",
      "Episode: 9, mean_r: -244.98009539459846, mean_episode: -198.41178480543465\n",
      "timesteps until break: 96\n",
      "Episode: 10, mean_r: -328.77245242970184, mean_episode: -211.4478515678614\n",
      "timesteps until break: 75\n",
      "Episode: 11, mean_r: -149.51069226829418, mean_episode: -205.8172007224462\n",
      "timesteps until break: 69\n",
      "Episode: 12, mean_r: -33.78338063654358, mean_episode: -191.48104904862097\n",
      "timesteps until break: 79\n",
      "Episode: 13, mean_r: -131.69659272966254, mean_episode: -186.8822447163934\n",
      "timesteps until break: 65\n",
      "Episode: 14, mean_r: -139.30643383722494, mean_episode: -183.4839725107385\n",
      "timesteps until break: 60\n",
      "Episode: 15, mean_r: -155.3210242804061, mean_episode: -181.60644262871634\n",
      "timesteps until break: 98\n",
      "Episode: 16, mean_r: 22.21247922178297, mean_episode: -168.86776001306015\n",
      "timesteps until break: 77\n",
      "Episode: 17, mean_r: -138.0835761988498, mean_episode: -167.05692567104776\n",
      "timesteps until break: 86\n",
      "Episode: 18, mean_r: -119.88025880871862, mean_episode: -164.4359997342517\n",
      "timesteps until break: 54\n",
      "Episode: 19, mean_r: -149.71002276167894, mean_episode: -163.6609483146426\n",
      "timesteps until break: 58\n",
      "Episode: 20, mean_r: -183.98578044464745, mean_episode: -164.67718992114285\n",
      "timesteps until break: 69\n",
      "Episode: 21, mean_r: -279.17599603484916, mean_episode: -170.12951402179553\n",
      "timesteps until break: 64\n",
      "Episode: 22, mean_r: -365.1698496680404, mean_episode: -178.99498382389757\n",
      "timesteps until break: 80\n",
      "Episode: 23, mean_r: -438.3366366313375, mean_episode: -190.27070785900366\n",
      "timesteps until break: 86\n",
      "Episode: 24, mean_r: -62.00810901759541, mean_episode: -184.92643290727833\n",
      "timesteps until break: 86\n",
      "Episode: 25, mean_r: -337.25017667621466, mean_episode: -191.01938265803577\n",
      "timesteps until break: 74\n",
      "Episode: 26, mean_r: -143.2402002983038, mean_episode: -189.18172179804606\n",
      "timesteps until break: 75\n",
      "Episode: 27, mean_r: -276.52973013628093, mean_episode: -192.41683321798067\n",
      "timesteps until break: 71\n",
      "Episode: 28, mean_r: -120.31175377696084, mean_episode: -189.84165180937285\n",
      "timesteps until break: 55\n",
      "Episode: 29, mean_r: -115.96917569979297, mean_episode: -187.2943250469735\n",
      "timesteps until break: 69\n",
      "Episode: 30, mean_r: -167.78444037156058, mean_episode: -186.6439955577931\n",
      "timesteps until break: 87\n",
      "Episode: 31, mean_r: -182.28314253894848, mean_episode: -186.50332287976585\n",
      "timesteps until break: 53\n",
      "Episode: 32, mean_r: -190.63258549309973, mean_episode: -186.63236233643255\n",
      "timesteps until break: 85\n",
      "Episode: 33, mean_r: -219.87365934104315, mean_episode: -187.63967436687528\n",
      "timesteps until break: 57\n",
      "Episode: 34, mean_r: -147.146132715748, mean_episode: -186.44868784772447\n",
      "timesteps until break: 87\n",
      "Episode: 35, mean_r: -102.0070482216116, mean_episode: -184.03606957269267\n",
      "timesteps until break: 89\n",
      "Episode: 36, mean_r: -140.79326024877614, mean_episode: -182.8348804248061\n",
      "timesteps until break: 62\n",
      "Episode: 37, mean_r: -123.23589731257738, mean_episode: -181.22409709744858\n",
      "timesteps until break: 90\n",
      "Episode: 38, mean_r: -185.7831904350919, mean_episode: -181.34407323791288\n",
      "timesteps until break: 58\n",
      "Episode: 39, mean_r: -111.791252044738, mean_episode: -179.560667566293\n",
      "timesteps until break: 84\n",
      "Episode: 40, mean_r: -523.5490894838829, mean_episode: -188.16037811423277\n",
      "timesteps until break: 76\n",
      "Episode: 41, mean_r: -139.9811003081293, mean_episode: -186.98527377749855\n",
      "timesteps until break: 79\n",
      "Episode: 42, mean_r: -148.8327778686002, mean_episode: -186.07688101776287\n",
      "timesteps until break: 61\n",
      "Episode: 43, mean_r: -125.32560798790485, mean_episode: -184.6640607147429\n",
      "timesteps until break: 86\n",
      "Episode: 44, mean_r: -439.9887336709789, mean_episode: -190.46689419102097\n",
      "timesteps until break: 68\n",
      "Episode: 45, mean_r: -142.23961540421317, mean_episode: -189.39517688464747\n",
      "timesteps until break: 55\n",
      "Episode: 46, mean_r: -124.61384378255914, mean_episode: -187.98688703460206\n",
      "timesteps until break: 84\n",
      "Episode: 47, mean_r: -424.02858570374855, mean_episode: -193.00905083607327\n",
      "timesteps until break: 70\n",
      "Episode: 48, mean_r: -163.9930764606857, mean_episode: -192.4045513699194\n",
      "timesteps until break: 72\n",
      "Episode: 49, mean_r: -121.69593186132104, mean_episode: -190.9615183187235\n",
      "timesteps until break: 60\n",
      "Episode: 50, mean_r: -298.4258161148797, mean_episode: -193.11080427464663\n",
      "timesteps until break: 84\n",
      "Episode: 51, mean_r: -393.88621090046786, mean_episode: -197.0475769535843\n",
      "timesteps until break: 59\n",
      "Episode: 52, mean_r: -227.38950282918557, mean_episode: -197.6310755281151\n",
      "timesteps until break: 63\n",
      "Episode: 53, mean_r: -275.5153159807867, mean_episode: -199.10058949892021\n",
      "timesteps until break: 69\n",
      "Episode: 54, mean_r: -359.3861211588127, mean_episode: -202.0688400852145\n",
      "timesteps until break: 68\n",
      "Episode: 55, mean_r: -382.13990989970955, mean_episode: -205.34285953638715\n",
      "timesteps until break: 82\n",
      "Episode: 56, mean_r: -139.90416365119907, mean_episode: -204.17431139558022\n",
      "timesteps until break: 75\n",
      "Episode: 57, mean_r: -427.8541878406495, mean_episode: -208.09851975426562\n",
      "timesteps until break: 60\n",
      "Episode: 58, mean_r: -133.15362106692118, mean_episode: -206.80636632862178\n",
      "timesteps until break: 51\n",
      "Episode: 59, mean_r: -105.4475174495908, mean_episode: -205.08841973745177\n",
      "timesteps until break: 55\n",
      "Episode: 60, mean_r: -223.92620141941487, mean_episode: -205.4023827654845\n",
      "timesteps until break: 72\n",
      "Episode: 61, mean_r: -434.0898244197749, mean_episode: -209.1513572188335\n",
      "timesteps until break: 72\n",
      "Episode: 62, mean_r: -447.90303250160486, mean_episode: -213.00219069113626\n",
      "timesteps until break: 53\n",
      "Episode: 63, mean_r: -296.6441314356391, mean_episode: -214.32984054422363\n",
      "timesteps until break: 63\n",
      "Episode: 64, mean_r: -442.9216043910978, mean_episode: -217.90158685433101\n",
      "timesteps until break: 60\n",
      "Episode: 65, mean_r: -170.2888818165089, mean_episode: -217.16908369990298\n",
      "timesteps until break: 80\n",
      "Episode: 66, mean_r: -495.3704766682694, mean_episode: -221.3842563206358\n",
      "timesteps until break: 57\n",
      "Episode: 67, mean_r: -270.23672942945666, mean_episode: -222.11339771031967\n",
      "timesteps until break: 60\n",
      "Episode: 68, mean_r: -271.6795746373915, mean_episode: -222.84231207689427\n",
      "timesteps until break: 77\n",
      "Episode: 69, mean_r: -168.76849356021722, mean_episode: -222.05863354766709\n",
      "timesteps until break: 56\n",
      "Episode: 70, mean_r: -281.59376146574846, mean_episode: -222.90913537506825\n",
      "timesteps until break: 76\n",
      "Episode: 71, mean_r: -309.9778599928386, mean_episode: -224.13545544010725\n",
      "timesteps until break: 84\n",
      "Episode: 72, mean_r: -177.19858501682842, mean_episode: -223.48355446200614\n",
      "timesteps until break: 58\n",
      "Episode: 73, mean_r: -290.5688265648303, mean_episode: -224.4025307921818\n",
      "timesteps until break: 89\n",
      "Episode: 74, mean_r: -300.07964634964935, mean_episode: -225.42519451593137\n",
      "timesteps until break: 69\n",
      "Episode: 75, mean_r: -217.56379062805303, mean_episode: -225.32037579742635\n",
      "timesteps until break: 76\n",
      "Episode: 76, mean_r: -243.68552473424336, mean_episode: -225.56202249396338\n",
      "timesteps until break: 88\n",
      "Episode: 77, mean_r: -482.38614407485255, mean_episode: -228.8974006963126\n",
      "timesteps until break: 53\n",
      "Episode: 78, mean_r: -263.1737955735667, mean_episode: -229.3368416562774\n",
      "timesteps until break: 57\n",
      "Episode: 79, mean_r: -234.01697148044957, mean_episode: -229.39608380595047\n",
      "timesteps until break: 68\n",
      "Episode: 80, mean_r: -165.79960799234212, mean_episode: -228.60112785828042\n",
      "timesteps until break: 88\n",
      "Episode: 81, mean_r: -444.78907310704966, mean_episode: -231.2701148366603\n",
      "timesteps until break: 55\n",
      "Episode: 82, mean_r: -113.70579208416397, mean_episode: -229.8364035835811\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "timesteps until break: 74\n",
      "Episode: 83, mean_r: -304.31521178830104, mean_episode: -230.73373862219216\n",
      "timesteps until break: 52\n",
      "Episode: 84, mean_r: -217.7429058670583, mean_episode: -230.5790858512977\n",
      "timesteps until break: 53\n",
      "Episode: 85, mean_r: -152.8862732675886, mean_episode: -229.66505276207758\n",
      "timesteps until break: 85\n",
      "Episode: 86, mean_r: -455.0312696590985, mean_episode: -232.2855901678569\n",
      "timesteps until break: 55\n",
      "Episode: 87, mean_r: -302.01250051544906, mean_episode: -233.0870489074844\n",
      "timesteps until break: 69\n",
      "Episode: 88, mean_r: -162.6765863403617, mean_episode: -232.28693001467616\n",
      "timesteps until break: 70\n",
      "Episode: 89, mean_r: -400.05838867876764, mean_episode: -234.1720025839356\n",
      "timesteps until break: 68\n",
      "Episode: 90, mean_r: -366.00445236711, mean_episode: -235.63680758152643\n",
      "timesteps until break: 51\n",
      "Episode: 91, mean_r: -265.61484783039214, mean_episode: -235.96623659525022\n",
      "timesteps until break: 62\n",
      "Episode: 92, mean_r: -338.0044282458472, mean_episode: -237.07534737406107\n",
      "timesteps until break: 77\n",
      "Episode: 93, mean_r: -505.5245020675199, mean_episode: -239.96189742452836\n",
      "timesteps until break: 70\n",
      "Episode: 94, mean_r: -382.6656036450871, mean_episode: -241.48002195878965\n",
      "timesteps until break: 58\n",
      "Episode: 95, mean_r: -258.6515078438946, mean_episode: -241.66077444179072\n",
      "timesteps until break: 72\n",
      "Episode: 96, mean_r: 54.26305913962469, mean_episode: -238.578234508651\n",
      "timesteps until break: 62\n",
      "Episode: 97, mean_r: -249.61095945551506, mean_episode: -238.69197394109293\n",
      "timesteps until break: 62\n",
      "Episode: 98, mean_r: -359.5476641708132, mean_episode: -239.92519526996762\n",
      "timesteps until break: 57\n",
      "Episode: 99, mean_r: -252.1135530718956, mean_episode: -240.0483099952396\n",
      "timesteps until break: 88\n",
      "Episode: 100, mean_r: -435.87500091944963, mean_episode: -242.0065769044817\n",
      "timesteps until break: 87\n",
      "Episode: 101, mean_r: -196.5953768783624, mean_episode: -242.47911351319146\n",
      "timesteps until break: 83\n",
      "Episode: 102, mean_r: -193.28383741884556, mean_episode: -243.14248947033101\n",
      "timesteps until break: 57\n",
      "Episode: 103, mean_r: -282.03755936165174, mean_episode: -244.7110261876322\n",
      "timesteps until break: 58\n",
      "Episode: 104, mean_r: -84.90987908175481, mean_episode: -245.22310633032296\n",
      "timesteps until break: 78\n",
      "Episode: 105, mean_r: -186.07295329052664, mean_episode: -246.3883006246829\n",
      "timesteps until break: 66\n",
      "Episode: 106, mean_r: -157.05572094043228, mean_episode: -244.9317523234017\n",
      "timesteps until break: 62\n",
      "Episode: 107, mean_r: -138.14391886119324, mean_episode: -242.9788548559267\n",
      "timesteps until break: 67\n",
      "Episode: 108, mean_r: -242.89278420830576, mean_episode: -241.40923752634933\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-117-9ccf41e92746>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepisodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimesteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-114-bceef921cf4b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, episodes, timesteps)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0;31m# fill replay buffer with 10 samples before updating the policy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-bceef921cf4b>\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    139\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;31m# update target networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 141\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_actor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    142\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcritic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_critic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-114-bceef921cf4b>\u001b[0m in \u001b[0;36mupdate_target\u001b[0;34m(self, online, target)\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mupdate_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0monline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m             \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopy_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mparameter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtau\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent.train(episodes=1000, timesteps=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
